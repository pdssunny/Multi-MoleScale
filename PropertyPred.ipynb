{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "29608ae5-714a-473f-9100-d4556decc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os.path as osp\n",
    "import GCL.losses as L\n",
    "import GCL.augmentors as A\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from GCL.eval import get_split, SVMEvaluator\n",
    "from GCL.models import DualBranchContrast\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, precision_recall_curve, auc\n",
    "from sklearn import metrics\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "13fc9f68-4bc5-407a-8a23-05d766c1b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cddc295-6bc0-4c2e-a7f7-c34a6ce8e0d2",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1f3af6b6-b415-4001-a397-085c83496953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "from rdkit import Chem\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "class MyOwnDataset(InMemoryDataset):\n",
    "    def __init__(self, root, file = None, transform=None, pre_transform=None):\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        self.file = file\n",
    "        # print(root) # MYdata\n",
    "        # print(self.data) # Data(x=[3, 1], edge_index=[2, 4], y=[3])\n",
    "        # print(self.slices) # defaultdict(<class 'dict'>, {'x': tensor([0, 3, 6]), 'edge_index': tensor([ 0,  4, 10]), 'y': tensor([0, 3, 6])})\n",
    "        # print(self.processed_paths[0]) # MYdata\\processed\\datas.pt\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # pass \n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['datas.pt']\n",
    "    def download(self):\n",
    "        pass\n",
    "    def process(self):\n",
    "        df = pd.read_csv(file)\n",
    "        smiles = df['SMILES'].tolist()\n",
    "        featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "        out = featurizer.featurize(smiles)\n",
    "\n",
    "        data_list = []\n",
    "        for i in range(len(out)):\n",
    "            edge_index = torch.tensor(out[i].edge_index)\n",
    "            x = torch.tensor(out[i].node_features, dtype=torch.float)\n",
    "            y = torch.tensor(df['LABEL'][i])\n",
    "            edge_node = torch.tensor(out[i].edge_features)\n",
    "            data = Data(x=x, edge_index=edge_index, y = y, edge_node = edge_node)\n",
    "            data_list.append(data)\n",
    "\n",
    "        # data_list = data_list.append(data)\n",
    "        if self.pre_filter is not None: \n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "        if self.pre_transform is not None: \n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "        data, slices = self.collate(data_list) \n",
    "        # print(data)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97c4a5-9259-47e0-b9da-8e07ca402815",
   "metadata": {},
   "source": [
    "Load graph contrastive learning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af9a9001-69f4-4ac0-9d18-5a258161b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(GConv, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(make_gin_conv(input_dim, hidden_dim))\n",
    "            else:\n",
    "                self.layers.append(make_gin_conv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        project_dim = hidden_dim * num_layers\n",
    "        self.project = torch.nn.Sequential(\n",
    "            nn.Linear(project_dim, project_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(project_dim, project_dim))\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        z = x\n",
    "        zs = []\n",
    "        for conv, bn in zip(self.layers, self.batch_norms):\n",
    "            z = conv(z, edge_index)\n",
    "            z = F.relu(z)\n",
    "            z = bn(z)\n",
    "            zs.append(z)\n",
    "        gs = [global_add_pool(z, batch) for z in zs]\n",
    "        z, g = [torch.cat(x, dim=1) for x in [zs, gs]]\n",
    "        return z, g\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, encoder, augmentor):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.augmentor = augmentor\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        aug1, aug2 = self.augmentor\n",
    "        x1, edge_index1, edge_weight1 = aug1(x, edge_index)\n",
    "        x2, edge_index2, edge_weight2 = aug2(x, edge_index)\n",
    "        z, g = self.encoder(x, edge_index, batch)\n",
    "        z1, g1 = self.encoder(x1, edge_index1, batch)\n",
    "        z2, g2 = self.encoder(x2, edge_index2, batch)\n",
    "        return z, g, z1, z2, g1, g2\n",
    "\n",
    "def gcl_train(encoder_model, contrast_model, dataloader, optimizer):\n",
    "    encoder_model.train()\n",
    "    epoch_loss = 0\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "\n",
    "        _, _, _, _, g1, g2 = encoder_model(data.x, data.edge_index.to(torch.long), data.batch)\n",
    "        g1, g2 = [encoder_model.encoder.project(g) for g in [g1, g2]]\n",
    "        loss = contrast_model(g1=g1, g2=g2, batch=data.batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss\n",
    "\n",
    "def gcl_test(encoder_model, dataloader):\n",
    "    encoder_model.eval()\n",
    "    x = []\n",
    "    y = []\n",
    "    for data in dataloader:\n",
    "        data = data.to(device)\n",
    "        if data.x is None:\n",
    "            num_nodes = data.batch.size(0)\n",
    "            data.x = torch.ones((num_nodes, 1), dtype=torch.float32, device=data.batch.device)\n",
    "        _, g, _, _, _, _ = encoder_model(data.x, data.edge_index.to(torch.long), data.batch)\n",
    "        x.append(g)\n",
    "        y.append(data.y)\n",
    "    x = torch.cat(x, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    #split = get_split(num_samples=x.size()[0], train_ratio=0.8, test_ratio=0.1)\n",
    "    #result = SVMEvaluator()(x, y, split)\n",
    "    #return result\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a401852-cc32-4867-8bc7-c63869d5bb90",
   "metadata": {},
   "source": [
    "Load the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "862ba0df-837e-41a0-8d0c-06b5837bf80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AutoTokenizer,BertModel,RobertaTokenizer,RobertaModel\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "model_ = BertModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "\n",
    "def smiles_to_vector(seq):\n",
    "    inputs = tokenizer_(seq, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499e5ec-e852-483d-94dd-fe39e177bfc9",
   "metadata": {},
   "source": [
    "co-attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "258fecbd-a7b4-40f6-9641-becd043b9dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Att(nn.Module):\n",
    "    def __init__(self, hid_dim, dropout):\n",
    "        super(Att, self).__init__()\n",
    "\n",
    "        self.linear_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.linear_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.linear_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.linear_merge = nn.Linear(hid_dim, hid_dim)\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        atted = self.att(v, k, q, mask).transpose(-1,-2)\n",
    "        atted = self.linear_merge(atted)\n",
    "\n",
    "        return atted\n",
    "\n",
    "    def att(self, value, key, query, mask):\n",
    "        d_k = query.size(-1)\n",
    "\n",
    "        scores = torch.matmul(\n",
    "            query, key.transpose(-2, -1)\n",
    "        ) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "        att_map = F.softmax(scores, dim=-1)\n",
    "        att_map = self.dropout(att_map)\n",
    "\n",
    "        return torch.matmul(att_map, value)\n",
    "\n",
    "class MHAtt(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout):   #128,4,0.1\n",
    "        super(MHAtt, self).__init__()\n",
    "\n",
    "        self.linear_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.linear_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.linear_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.linear_merge = nn.Linear(hid_dim, hid_dim)\n",
    "        self.hid_dim = hid_dim\n",
    "        self.dropout = dropout\n",
    "        self.nhead = n_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size_head = int(self.hid_dim / self.nhead)\n",
    "    def forward(self, v, k, q, mask):\n",
    "        n_batches = q.size(0)\n",
    "\n",
    "        v = self.linear_v(v).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.nhead,\n",
    "            self.hidden_size_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        k = self.linear_k(k).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.nhead,\n",
    "            self.hidden_size_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        q = self.linear_q(q).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.nhead,\n",
    "            self.hidden_size_head\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        atted = self.att(v, k, q, mask)\n",
    "        atted = atted.transpose(1, 2).contiguous().view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.hid_dim\n",
    "        )\n",
    "\n",
    "        atted = self.linear_merge(atted)\n",
    "\n",
    "        return atted\n",
    "\n",
    "    def att(self, value, key, query, mask):\n",
    "        d_k = query.size(-1)\n",
    "\n",
    "        scores = torch.matmul(\n",
    "            query, key.transpose(-2, -1)\n",
    "        ) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "        att_map = F.softmax(scores, dim=-1)\n",
    "        att_map = self.dropout(att_map)\n",
    "\n",
    "        return torch.matmul(att_map, value)\n",
    "\n",
    "class SA(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout):\n",
    "        super(SA, self).__init__()\n",
    "\n",
    "        self.mhatt1 = MHAtt(hid_dim, n_heads, dropout)\n",
    "        # self.mhatt1 = MultiAttn(hid_dim, n_heads)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(hid_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        x = self.norm1(x + self.dropout1(\n",
    "            self.mhatt1(x, x, x, mask)\n",
    "        ))\n",
    "        # x = self.norm1(x + self.dropout1(\n",
    "        #     self.mhatt1(x, x, mask, mask)\n",
    "        # ))\n",
    "\n",
    "        return x\n",
    "\n",
    "class GSA(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout):\n",
    "        super(GSA, self).__init__()\n",
    "        self.mhatt1 = MHAtt(hid_dim, n_heads, dropout)\n",
    "        # self.mhatt1 = MultiAttn(hid_dim, n_heads)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(hid_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, y_mask=None):\n",
    "\n",
    "        # x as V while y as Q and K\n",
    "        # x = self.norm1(x + self.dropout1(\n",
    "        #     self.mhatt1(x, x, y, y_mask)\n",
    "        # ))\n",
    "        x = self.norm1(x+self.dropout1(\n",
    "            self.mhatt1(y, y, x, y_mask)\n",
    "        ))\n",
    "        # x = self.norm1(x + self.dropout1(\n",
    "        #     self.mhatt1(x, y, y_mask, y_mask)\n",
    "        # ))\n",
    "\n",
    "        return x\n",
    "\n",
    "class inter_cross_att(nn.Module):\n",
    "    def __init__(self, dim, nhead, dropout):\n",
    "        super(inter_cross_att, self).__init__()\n",
    "        self.gsa = SA(dim, nhead, dropout)\n",
    "        self.sga = SA(dim, nhead, dropout)\n",
    "        self.coa_gs = GSA(dim, nhead, dropout)\n",
    "        self.coa_sg = GSA(dim, nhead, dropout)\n",
    "\n",
    "    def forward(self, graph_vector, sequence_vector):\n",
    "        graph_vector = self.gsa(graph_vector, None)  # self-attention\n",
    "        sequence_vector = self.sga(sequence_vector, None)  # self-attention\n",
    "        graph_covector = self.coa_gs(graph_vector, sequence_vector, None)  # co-attention\n",
    "        sequence_covector = self.coa_sg(sequence_vector, graph_vector, None)  # co-attention\n",
    "\n",
    "        return graph_covector, sequence_covector\n",
    "\n",
    "class GSC(nn.Module):\n",
    "    def __init__(self, dim, nhead=2, dropout=0.1, layer_output =3, layer_coa=1):\n",
    "        super(GSC, self).__init__()\n",
    "        self.layer_output = layer_output\n",
    "        self.layer_coa = layer_coa\n",
    "        self.lin1 = nn.Linear(64, 128)\n",
    "        self.lin2 = nn.Linear(384, 128)\n",
    "        \n",
    "        self.sca_1 = SA(dim, nhead, dropout)\n",
    "        self.sca_2 = SA(dim, nhead, dropout)\n",
    "        self.sca_3 = SA(dim, nhead, dropout)\n",
    "\n",
    "        # self-protein-attention layers\n",
    "        self.spa_1 = SA(dim, nhead, dropout)\n",
    "        self.spa_2 = SA(dim, nhead, dropout)\n",
    "        self.spa_3 = SA(dim, nhead, dropout)\n",
    "\n",
    "\n",
    "        self.coa_gs_1 = GSA(dim, nhead, dropout)\n",
    "        self.coa_gs_2 = GSA(dim, nhead, dropout)\n",
    "        self.coa_sg_1 = GSA(dim, nhead, dropout)\n",
    "        self.coa_sg_2 = GSA(dim, nhead, dropout)\n",
    "        self.coa_gs_3 = GSA(dim, nhead, dropout)\n",
    "        self.coa_sg_3 = GSA(dim, nhead, dropout)\n",
    "\n",
    "        self.W_out = nn.ModuleList([nn.Linear(2 * dim, dim),nn.Linear(dim, 128),nn.Linear(128, 64)])\n",
    "        self.inter_coa_layers = nn.ModuleList([inter_cross_att(dim, nhead, dropout) for _ in range(layer_coa)])\n",
    "        self.W_interaction = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, X1, X2, y):\n",
    "\n",
    "        sequence_vector = self.lin1(X1)\n",
    "        graph_vector = self.lin2(X2)\n",
    "        for i in range(self.layer_coa):\n",
    "            sequence_vector, graph_vector = self.inter_coa_layers[i](sequence_vector, graph_vector)\n",
    "        graph_vector = graph_vector.mean(dim=1)\n",
    "        sequence_vector = sequence_vector.mean(dim=1)\n",
    "    \n",
    "        \"\"\"Concatenate the above two vectors and output the interaction.\"\"\"\n",
    "        cat_vector = torch.cat((sequence_vector, graph_vector), 1)\n",
    "        for j in range(self.layer_output):\n",
    "            cat_vector = torch.relu(self.W_out[j](cat_vector))\n",
    "        interaction = self.W_interaction(cat_vector)\n",
    "        return interaction\n",
    "\n",
    "    def __call__(self, X1, X2, y, train=True):\n",
    "\n",
    "        correct_interaction = y\n",
    "        predicted_interaction = self.forward(X1, X2, y)\n",
    "        if train:\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            loss = criterion(predicted_interaction, correct_interaction)\n",
    "            return loss, predicted_interaction\n",
    "        else:\n",
    "            correct_labels = correct_interaction.to(device).data.numpy()\n",
    "            ys = F.softmax(predicted_interaction, 1).to(device).data.numpy()\n",
    "            predicted_labels = list(map(lambda x: np.argmax(x), ys))\n",
    "            predicted_scores = list(map(lambda x: x[1], ys))\n",
    "            return correct_labels, predicted_labels, predicted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "232f2f87-f2e7-4432-b8ee-73077947372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer):\n",
    "    print('Training on {} samples...'.format(len(dataloader.dataset)))\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    for batch_idx, (x1,x2,y) in enumerate(dataloader):\n",
    "        #self.model(data.to(device), proteins.to(device), train=False)\n",
    "        optimizer.zero_grad()\n",
    "        sigma = model.forward(x1, x2, y)\n",
    "        loss = criterion(sigma, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 30 == 0:\n",
    "            print('Train epoch: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(batch_idx * len(x1),\n",
    "                                                                        len(dataloader.dataset),\n",
    "                                                                        100. * batch_idx / len(dataloader),\n",
    "                                                                        loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9f81e75-44b6-4382-9c1f-440271ae029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    T, Y, S = [], [], []\n",
    "    print('Make prediction for {} samples...'.format(len(testloader.dataset)))\n",
    "        \n",
    "    with torch.no_grad():  # accelerating calculations\n",
    "        for x1,x2,y in testloader:\n",
    "            (correct_labels, predicted_labels, predicted_scores) = model(x1, x2, y, train=False)\n",
    "            T.extend(correct_labels)\n",
    "            Y.extend(predicted_labels)\n",
    "            S.extend(predicted_scores)\n",
    "    tpr, fpr, _ = precision_recall_curve(T, S)\n",
    "    PRC = auc(fpr, tpr)\n",
    "    train_accu = metrics.accuracy_score(T, Y)\n",
    "    AUC = roc_auc_score(T, S)\n",
    "    precision = precision_score(T, Y)\n",
    "    recall = recall_score(T, Y)\n",
    "    return AUC, precision, recall, train_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b73a4-e342-4f26-b616-4618cc4bfe06",
   "metadata": {},
   "source": [
    "Take the BACE dataset as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1b86863a-89b1-4a1a-832b-ff107f996961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1513, 64])\n",
      "torch.Size([1513])\n"
     ]
    }
   ],
   "source": [
    "gcl_model = torch.load(\"gclModel\")   #Load the pre-trained graph contrastive learning model\n",
    "\n",
    "device = torch.device('cpu')\n",
    "path = \"BACE\"\n",
    "file = \"bace.csv\"\n",
    "gcl_dataset = MyOwnDataset(path, file)\n",
    "gcl_dataloader = DataLoader(gcl_dataset, batch_size=32)\n",
    "X,Y = gcl_test(gcl_model, gcl_dataloader)   # Obtain the embeddings generated by the pre-trained contrastive learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6be8d310-fcf1-4e29-a3a0-5d5a884d8f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file)\n",
    "bertdata = []\n",
    "for i in range(len(df)):\n",
    "    smiles_vector = smiles_to_vector(df.iloc[i,0]).tolist()\n",
    "    bertdata.append(smiles_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9fdc985d-2db8-40ca-9a9b-07d333d9df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCL_X = torch.tensor(X.detach().numpy()).to(torch.float32)\n",
    "GCL_y = torch.tensor(Y.detach().numpy()).to(torch.long)\n",
    "BERT_X = torch.tensor(bertdata).to(torch.float32)\n",
    "\n",
    "data = TensorDataset(GCL_X, BERT_X, GCL_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c0799eec-43b7-4058-b452-d04257bf1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets, with 80% for training, 10% for validation, and 10% for testing.\n",
    "train_size = int(0.8 * len(data))\n",
    "temp_size = len(data) - train_size\n",
    "train_dataset, temp_dataset = random_split(data, [train_size, temp_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "valid_size = int(0.5 * len(temp_dataset))\n",
    "test_size = len(temp_dataset) - valid_size\n",
    "valid_dataset, test_dataset = random_split(temp_dataset, [valid_size, test_size])\n",
    "\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d62ac-e9de-4016-8261-e7c900950c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.691195\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.696451\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.6430769230769231 , precision :  0.0 , recall :  0.0 , acc :  0.6578947368421053\n",
      "0 best AUC :  0.6430769230769231 best precision :  0.0 best recall :  0.0 best acc :  0.6578947368421053\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.673083\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.684838\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.7669230769230769 , precision :  0.4065040650406504 , recall :  0.9615384615384616 , acc :  0.506578947368421\n",
      "1 best AUC :  0.7669230769230769 best precision :  0.4065040650406504 best recall :  0.9615384615384616 best acc :  0.506578947368421\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.672480\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.714493\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.7592307692307693 , precision :  0.5121951219512195 , recall :  0.8076923076923077 , acc :  0.6710526315789473\n",
      "2 best AUC :  0.7669230769230769 best precision :  0.4065040650406504 best recall :  0.9615384615384616 best acc :  0.506578947368421\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.448204\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.553520\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.7826923076923077 , precision :  0.5066666666666667 , recall :  0.7307692307692307 , acc :  0.6644736842105263\n",
      "3 best AUC :  0.7826923076923077 best precision :  0.5066666666666667 best recall :  0.7307692307692307 best acc :  0.6644736842105263\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.692256\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.487138\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.7999999999999999 , precision :  0.46875 , recall :  0.8653846153846154 , acc :  0.618421052631579\n",
      "4 best AUC :  0.7999999999999999 best precision :  0.46875 best recall :  0.8653846153846154 best acc :  0.618421052631579\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.666462\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.604615\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.8030769230769231 , precision :  0.5774647887323944 , recall :  0.7884615384615384 , acc :  0.7302631578947368\n",
      "5 best AUC :  0.8030769230769231 best precision :  0.5774647887323944 best recall :  0.7884615384615384 best acc :  0.7302631578947368\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.576532\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.560752\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.8255769230769231 , precision :  0.6440677966101694 , recall :  0.7307692307692307 , acc :  0.7697368421052632\n",
      "6 best AUC :  0.8255769230769231 best precision :  0.6440677966101694 best recall :  0.7307692307692307 best acc :  0.7697368421052632\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.581952\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.556317\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.8357692307692307 , precision :  0.5443037974683544 , recall :  0.8269230769230769 , acc :  0.7039473684210527\n",
      "7 best AUC :  0.8357692307692307 best precision :  0.5443037974683544 best recall :  0.8269230769230769 best acc :  0.7039473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.585334\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.624924\n",
      "Make prediction for 151 samples...\n",
      "8 best AUC :  0.8357692307692307 best precision :  0.5443037974683544 best recall :  0.8269230769230769 best acc :  0.7039473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.449930\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.512661\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.8474999999999999 , precision :  0.5294117647058824 , recall :  0.8653846153846154 , acc :  0.6907894736842105\n",
      "9 best AUC :  0.8474999999999999 best precision :  0.5294117647058824 best recall :  0.8653846153846154 best acc :  0.6907894736842105\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.449676\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.414356\n",
      "Make prediction for 151 samples...\n",
      "10 best AUC :  0.8474999999999999 best precision :  0.5294117647058824 best recall :  0.8653846153846154 best acc :  0.6907894736842105\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.470546\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.416100\n",
      "Make prediction for 151 samples...\n",
      "11 best AUC :  0.8474999999999999 best precision :  0.5294117647058824 best recall :  0.8653846153846154 best acc :  0.6907894736842105\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.468236\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.448272\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.8715384615384616 , precision :  0.6285714285714286 , recall :  0.8461538461538461 , acc :  0.7763157894736842\n",
      "12 best AUC :  0.8715384615384616 best precision :  0.6285714285714286 best recall :  0.8461538461538461 best acc :  0.7763157894736842\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.409410\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.407876\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.8815384615384615 , precision :  0.6833333333333333 , recall :  0.7884615384615384 , acc :  0.8026315789473685\n",
      "13 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.415587\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.417833\n",
      "Make prediction for 151 samples...\n",
      "14 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.542161\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.401422\n",
      "Make prediction for 151 samples...\n",
      "15 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.383997\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.418000\n",
      "Make prediction for 151 samples...\n",
      "16 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.323077\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.303139\n",
      "Make prediction for 151 samples...\n",
      "17 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.440405\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.405885\n",
      "Make prediction for 151 samples...\n",
      "18 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.315390\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.435367\n",
      "Make prediction for 151 samples...\n",
      "19 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.516152\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.677527\n",
      "Make prediction for 151 samples...\n",
      "20 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.425635\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.330801\n",
      "Make prediction for 151 samples...\n",
      "21 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.360134\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.359068\n",
      "Make prediction for 151 samples...\n",
      "22 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.430278\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.487191\n",
      "Make prediction for 151 samples...\n",
      "23 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.354761\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.413474\n",
      "Make prediction for 151 samples...\n",
      "24 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.339627\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.457243\n",
      "Make prediction for 151 samples...\n",
      "25 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.747661\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.327023\n",
      "Make prediction for 151 samples...\n",
      "26 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.379715\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.302898\n",
      "Make prediction for 151 samples...\n",
      "27 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.432554\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.344671\n",
      "Make prediction for 151 samples...\n",
      "28 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.595657\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.555159\n",
      "Make prediction for 151 samples...\n",
      "29 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.429308\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.269203\n",
      "Make prediction for 151 samples...\n",
      "30 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.438321\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.424870\n",
      "Make prediction for 151 samples...\n",
      "31 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.486643\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.396324\n",
      "Make prediction for 151 samples...\n",
      "32 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.631505\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.311717\n",
      "Make prediction for 151 samples...\n",
      "33 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.511221\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.350450\n",
      "Make prediction for 151 samples...\n",
      "34 best AUC :  0.8815384615384615 best precision :  0.6833333333333333 best recall :  0.7884615384615384 best acc :  0.8026315789473685\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.361326\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.484264\n",
      "Make prediction for 151 samples...\n",
      "Make prediction for 152 samples...\n",
      "AUC :  0.8951923076923077 , precision :  0.7407407407407407 , recall :  0.7692307692307693 , acc :  0.8289473684210527\n",
      "35 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.324850\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.441961\n",
      "Make prediction for 151 samples...\n",
      "36 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.267877\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.394991\n",
      "Make prediction for 151 samples...\n",
      "37 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.464544\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.424608\n",
      "Make prediction for 151 samples...\n",
      "38 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.302881\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.507371\n",
      "Make prediction for 151 samples...\n",
      "39 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.324904\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.480353\n",
      "Make prediction for 151 samples...\n",
      "40 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.354697\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.319677\n",
      "Make prediction for 151 samples...\n",
      "41 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.348857\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.382171\n",
      "Make prediction for 151 samples...\n",
      "42 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.339047\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.424259\n",
      "Make prediction for 151 samples...\n",
      "43 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.278764\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.437335\n",
      "Make prediction for 151 samples...\n",
      "44 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.313336\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.385486\n",
      "Make prediction for 151 samples...\n",
      "45 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.307152\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.510748\n",
      "Make prediction for 151 samples...\n",
      "46 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.444499\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.242359\n",
      "Make prediction for 151 samples...\n",
      "47 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.423915\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.498069\n",
      "Make prediction for 151 samples...\n",
      "48 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.469345\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.326110\n",
      "Make prediction for 151 samples...\n",
      "49 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.247710\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.306684\n",
      "Make prediction for 151 samples...\n",
      "50 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.369352\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.350001\n",
      "Make prediction for 151 samples...\n",
      "51 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.260797\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.380159\n",
      "Make prediction for 151 samples...\n",
      "52 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.232533\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.292996\n",
      "Make prediction for 151 samples...\n",
      "53 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.353162\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.306625\n",
      "Make prediction for 151 samples...\n",
      "54 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.248083\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.287380\n",
      "Make prediction for 151 samples...\n",
      "55 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.337842\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.383929\n",
      "Make prediction for 151 samples...\n",
      "56 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.195220\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.295057\n",
      "Make prediction for 151 samples...\n",
      "57 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.302170\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.243833\n",
      "Make prediction for 151 samples...\n",
      "58 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.339020\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.302076\n",
      "Make prediction for 151 samples...\n",
      "59 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.200699\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.345312\n",
      "Make prediction for 151 samples...\n",
      "60 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.251708\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.180719\n",
      "Make prediction for 151 samples...\n",
      "61 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.386091\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.294722\n",
      "Make prediction for 151 samples...\n",
      "62 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.400991\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.203601\n",
      "Make prediction for 151 samples...\n",
      "63 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.330567\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.272343\n",
      "Make prediction for 151 samples...\n",
      "64 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.227022\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.311755\n",
      "Make prediction for 151 samples...\n",
      "65 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.193975\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.219229\n",
      "Make prediction for 151 samples...\n",
      "66 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.326479\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.306874\n",
      "Make prediction for 151 samples...\n",
      "67 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.304635\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.141739\n",
      "Make prediction for 151 samples...\n",
      "68 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.270914\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.365460\n",
      "Make prediction for 151 samples...\n",
      "69 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.284694\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.191317\n",
      "Make prediction for 151 samples...\n",
      "70 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.298925\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.394974\n",
      "Make prediction for 151 samples...\n",
      "71 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.357589\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.236963\n",
      "Make prediction for 151 samples...\n",
      "72 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.259677\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.220992\n",
      "Make prediction for 151 samples...\n",
      "73 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.269023\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.351525\n",
      "Make prediction for 151 samples...\n",
      "74 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.176811\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.208766\n",
      "Make prediction for 151 samples...\n",
      "75 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.199342\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.425499\n",
      "Make prediction for 151 samples...\n",
      "76 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.446029\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.272909\n",
      "Make prediction for 151 samples...\n",
      "77 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.192862\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.154507\n",
      "Make prediction for 151 samples...\n",
      "78 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.157187\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.228296\n",
      "Make prediction for 151 samples...\n",
      "79 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.153329\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.247672\n",
      "Make prediction for 151 samples...\n",
      "80 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.208852\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.163467\n",
      "Make prediction for 151 samples...\n",
      "81 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.313641\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.216244\n",
      "Make prediction for 151 samples...\n",
      "82 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.231487\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.182020\n",
      "Make prediction for 151 samples...\n",
      "83 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.247188\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.181279\n",
      "Make prediction for 151 samples...\n",
      "84 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.148875\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.143666\n",
      "Make prediction for 151 samples...\n",
      "85 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.202945\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.108422\n",
      "Make prediction for 151 samples...\n",
      "86 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.300662\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.304705\n",
      "Make prediction for 151 samples...\n",
      "87 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.353759\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.303493\n",
      "Make prediction for 151 samples...\n",
      "88 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.130579\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.369673\n",
      "Make prediction for 151 samples...\n",
      "89 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.193080\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.372578\n",
      "Make prediction for 151 samples...\n",
      "90 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.182794\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.270266\n",
      "Make prediction for 151 samples...\n",
      "91 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.197270\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.217102\n",
      "Make prediction for 151 samples...\n",
      "92 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.103743\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.226885\n",
      "Make prediction for 151 samples...\n",
      "93 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.219807\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.300771\n",
      "Make prediction for 151 samples...\n",
      "94 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.171011\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.179463\n",
      "Make prediction for 151 samples...\n",
      "95 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.180618\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.259864\n",
      "Make prediction for 151 samples...\n",
      "96 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.262223\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.157763\n",
      "Make prediction for 151 samples...\n",
      "97 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.168417\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.213522\n",
      "Make prediction for 151 samples...\n",
      "98 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.126181\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.245092\n",
      "Make prediction for 151 samples...\n",
      "99 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.142342\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.165747\n",
      "Make prediction for 151 samples...\n",
      "100 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.163322\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.264056\n",
      "Make prediction for 151 samples...\n",
      "101 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.154900\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.219683\n",
      "Make prediction for 151 samples...\n",
      "102 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.129174\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.274719\n",
      "Make prediction for 151 samples...\n",
      "103 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.140868\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.309102\n",
      "Make prediction for 151 samples...\n",
      "104 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.143859\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.234810\n",
      "Make prediction for 151 samples...\n",
      "105 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.239012\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.408769\n",
      "Make prediction for 151 samples...\n",
      "106 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.203299\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.292201\n",
      "Make prediction for 151 samples...\n",
      "107 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.147897\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.157690\n",
      "Make prediction for 151 samples...\n",
      "108 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.205695\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.243481\n",
      "Make prediction for 151 samples...\n",
      "109 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.385058\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.153458\n",
      "Make prediction for 151 samples...\n",
      "110 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.158431\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.089060\n",
      "Make prediction for 151 samples...\n",
      "111 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.109400\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.123651\n",
      "Make prediction for 151 samples...\n",
      "112 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.137634\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.087592\n",
      "Make prediction for 151 samples...\n",
      "113 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.155665\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.225012\n",
      "Make prediction for 151 samples...\n",
      "114 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.115779\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.155915\n",
      "Make prediction for 151 samples...\n",
      "115 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.136965\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.189098\n",
      "Make prediction for 151 samples...\n",
      "116 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.128188\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.235285\n",
      "Make prediction for 151 samples...\n",
      "117 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.130509\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.247427\n",
      "Make prediction for 151 samples...\n",
      "118 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.330826\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.163197\n",
      "Make prediction for 151 samples...\n",
      "119 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.417644\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.177021\n",
      "Make prediction for 151 samples...\n",
      "120 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.110791\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.090713\n",
      "Make prediction for 151 samples...\n",
      "121 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.131159\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.199953\n",
      "Make prediction for 151 samples...\n",
      "122 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.157219\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.170263\n",
      "Make prediction for 151 samples...\n",
      "123 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.089484\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.154063\n",
      "Make prediction for 151 samples...\n",
      "124 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.129818\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.322820\n",
      "Make prediction for 151 samples...\n",
      "125 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.197956\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.229471\n",
      "Make prediction for 151 samples...\n",
      "126 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.039985\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.214187\n",
      "Make prediction for 151 samples...\n",
      "127 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.161348\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.164915\n",
      "Make prediction for 151 samples...\n",
      "128 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.119124\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.204340\n",
      "Make prediction for 151 samples...\n",
      "129 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.098816\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.335561\n",
      "Make prediction for 151 samples...\n",
      "130 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.246684\n",
      "Train epoch: [960/1210 (79%)]\tLoss: 0.244819\n",
      "Make prediction for 151 samples...\n",
      "131 best AUC :  0.8951923076923077 best precision :  0.7407407407407407 best recall :  0.7692307692307693 best acc :  0.8289473684210527\n",
      "Training on 1210 samples...\n",
      "Train epoch: [0/1210 (0%)]\tLoss: 0.086878\n"
     ]
    }
   ],
   "source": [
    "model = GSC(dim = 128, nhead = 8, dropout=0.1, layer_output = 3, layer_coa = 1)\n",
    "lr = 0.0005\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion1 = torch.nn.CrossEntropyLoss()\n",
    "validAUC = 0.0\n",
    "bestAUC = 0.0\n",
    "bestprecision = 0.0\n",
    "bestrecall = 0.0\n",
    "bestacc = 0.0\n",
    "for epoch in range(200):\n",
    "    train(model,train_loader, opt)\n",
    "    AUC, _, _, _ = test(model,valid_loader)\n",
    "    if AUC > validAUC:\n",
    "        validAUC = AUC\n",
    "        testAUC, testPRE, testRECALL, testACC = test(model,test_loader)\n",
    "        print(\"AUC : \", testAUC, \", precision : \", testPRE, \", recall : \",  testRECALL, \", acc : \", testACC)\n",
    "        if testAUC > bestAUC:\n",
    "            bestAUC = testAUC\n",
    "            bestprecision = testPRE\n",
    "            bestrecall = testRECALL\n",
    "            bestacc = testACC\n",
    "    print(epoch, \"best AUC : \", bestAUC, \"best precision : \", bestprecision, \"best recall : \", bestrecall, \"best acc : \", bestacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fcef53-c3c4-40d7-a797-e393b899485f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph111",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
